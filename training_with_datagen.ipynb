{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X39ufJMWhLeP"
   },
   "source": [
    "# Détection de gravure par Deep Learning\n",
    "### But : détecter toutes les gravures sur une image (pas de différences sémantiques)\n",
    "\n",
    "On donne une carte de normale en entrée au réseau\n",
    "\n",
    "Le dataset est générer dynamiquement par un générateur (les transformations également). Il suffit de donner les relevés, les cartes de normales et les masques de placement en paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1750253251651,
     "user": {
      "displayName": "Nino Rottier",
      "userId": "12165733875658235380"
     },
     "user_tz": -120
    },
    "id": "YIEzoMfBORmZ",
    "outputId": "07d15406-50ab-4456-cddd-ee702da40e0d"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta76gfjtGKmF"
   },
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_to_search = \"./dataset/data_nino_limeuil/normale\"\n",
    "\n",
    "DATA_NAMES = [f for f in os.listdir(folder_to_search) if f.endswith(\".png\")]\n",
    "# DATA_NAMES = ['os_0.png', 'os_1.png', 'os_5.png', 'os_2.png', 'os_3.png']\n",
    "print(f\"Found {len(DATA_NAMES)} images in {folder_to_search}\")\n",
    "print(DATA_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1750253273444,
     "user": {
      "displayName": "Nino Rottier",
      "userId": "12165733875658235380"
     },
     "user_tz": -120
    },
    "id": "YGMtKwmaG0AJ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCH_SIZE = 40\n",
    "PATCH_SIZE = 512\n",
    "IMG_FOLDER = \"./dataset/data_nino_limeuil/normale\"\n",
    "GROUNDTRUTH_FOLDER = \"./dataset/data_nino_limeuil/gravure\"\n",
    "PATCH_RATIO = 0.5 # for gravure dilation -> patch selection\n",
    "ROTATION_STEP = 10\n",
    "NOISE_SCALE = 64\n",
    "NOISE_MAX_ANGLE = 5\n",
    "RESCALE = 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24312,
     "status": "ok",
     "timestamp": 1750253879090,
     "user": {
      "displayName": "Nino Rottier",
      "userId": "12165733875658235380"
     },
     "user_tz": -120
    },
    "id": "YtwlSXpxNcLR"
   },
   "outputs": [],
   "source": [
    "from src.dataGenerator import DataGenerator\n",
    "\n",
    "gen = DataGenerator(\n",
    "    DATA_NAMES, \n",
    "    BATCH_SIZE, \n",
    "    EPOCH_SIZE, \n",
    "    PATCH_SIZE, \n",
    "    IMG_FOLDER, \n",
    "    GROUNDTRUTH_FOLDER, \n",
    "    PATCH_RATIO, \n",
    "    ROTATION_STEP, \n",
    "    NOISE_SCALE,\n",
    "    NOISE_MAX_ANGLE,\n",
    "    RESCALE,\n",
    "    flip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-KmOKv-yrD4"
   },
   "source": [
    "### Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = PATCH_SIZE\n",
    "IMG_WIDTH = PATCH_SIZE\n",
    "IMG_CHANNELS = 3  # 1 for grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1750255268573,
     "user": {
      "displayName": "Nino Rottier",
      "userId": "12165733875658235380"
     },
     "user_tz": -120
    },
    "id": "JOu5Lp3aN1C5",
    "outputId": "e486143b-f873-4c49-f26d-af6b1d1628ba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#TODO: Essayer d'ajouter une fuzzy layer au début pour gérer les bords de gravure un peu flous\n",
    "#TODO: Ajouter une couche d'attention ?\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import MaxPooling2D, Conv2DTranspose\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    # For 2D RGB input (batch, height, width, channels), use Conv2D, MaxPooling2D, etc.\n",
    "    # receptive field ~= 100x100\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p1)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p2)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p3)\n",
    "    b = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(b)\n",
    "\n",
    "    # Upsampling\n",
    "    u3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(b)\n",
    "    u3 = concatenate([u3, c3])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u3)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c4)\n",
    "\n",
    "    u2 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(c4)\n",
    "    u2 = concatenate([u2, c2])\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u2)\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c5)\n",
    "\n",
    "    u1 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(c5)\n",
    "    u1 = concatenate([u1, c1])\n",
    "    c6 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u1)\n",
    "    c6 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c6)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid', kernel_initializer='he_normal')(c6)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def build_unet_big(input_shape):\n",
    "    # receptive field ~= 800x800 with 6 downsampling layers \n",
    "    # ~ 400x400 with 5\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    # Downsampling\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p1)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p2)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p3)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p4)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c5)\n",
    "    p5 = MaxPooling2D((2, 2))(c5)\n",
    "\n",
    "    # c6 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p5)\n",
    "    # c6 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c6)\n",
    "    # p6 = MaxPooling2D((2, 2))(c6)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = Conv2D(1024, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(p5)\n",
    "    b = Conv2D(1024, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(b)\n",
    "\n",
    "    # Upsampling\n",
    "    # u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(b)\n",
    "    # u6 = concatenate([u6, c6])\n",
    "    # c7 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u6)\n",
    "    # c7 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c7)\n",
    "\n",
    "    u5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(b)\n",
    "    u5 = concatenate([u5, c5])\n",
    "    c8 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u5)\n",
    "    c8 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c8)\n",
    "\n",
    "    u4 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(c8)\n",
    "    u4 = concatenate([u4, c4])\n",
    "    c9 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u4)\n",
    "    c9 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c9)\n",
    "\n",
    "    u3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(c9)\n",
    "    u3 = concatenate([u3, c3])\n",
    "    c10 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u3)\n",
    "    c10 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c10)\n",
    "\n",
    "    u2 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(c10)\n",
    "    u2 = concatenate([u2, c2])\n",
    "    c11 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u2)\n",
    "    c11 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c11)\n",
    "\n",
    "    u1 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same', kernel_initializer='he_normal')(c11)\n",
    "    u1 = concatenate([u1, c1])\n",
    "    c12 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(u1)\n",
    "    c12 = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(c12)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid', kernel_initializer='he_normal')(c12)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model = build_unet_big((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Champ réceptif du modèle\n",
    "\n",
    "Utiliser juste pour vérifier le modèle (Attention, cela réinitialise les poids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser uniquement les coefficients (poids) du modèle à 1, laisser les biais inchangés\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'kernel_initializer'):\n",
    "        weights = layer.get_weights()\n",
    "        if weights:\n",
    "            # Généralement, le premier élément est le kernel, le second le biais (si présent)\n",
    "            new_weights = []\n",
    "            for i, w in enumerate(weights):\n",
    "                if i == 0:  # kernel/coefficients\n",
    "                    new_weights.append(np.ones(w.shape, dtype=w.dtype))\n",
    "                else:  # biais ou autres\n",
    "                    new_weights.append(w)\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "# Créer une image noire avec un pixel blanc au centre\n",
    "input_img = np.zeros((1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
    "center_h = IMG_HEIGHT // 2\n",
    "center_w = IMG_WIDTH // 2\n",
    "input_img[0, center_h, center_w, :] = 1.0\n",
    "\n",
    "# Prédiction\n",
    "output = model.predict(input_img)[0, ..., 0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(output, cmap='hot')\n",
    "plt.title(\"Sortie du modèle (champ réceptif)\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNF51CcodoKb"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "def dice_loss(y_true, y_pred, smooth=1e-8):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true*y_pred) #, axis=[1,2,3])\n",
    "    \n",
    "    # union = tf.reduce_sum(y_true + y_pred, axis=[1,2,3])\n",
    "    # dice = (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    den1 = tf.reduce_sum(y_true * y_true) #, axis=[1, 2, 3])\n",
    "    den2 = tf.reduce_sum(y_pred * y_pred) #, axis=[1, 2, 3])\n",
    "    dice = (2. * intersection + smooth) / (den1 + den2 + smooth)\n",
    "\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return bce + dice\n",
    "\n",
    "def focal_dice_loss(y_true, y_pred, lambda_bfce=1.0):\n",
    "    bfce = tf.keras.losses.BinaryFocalCrossentropy()(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return lambda_bfce * bfce + dice\n",
    "\n",
    "# Metrics\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "    return f1\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(filepath=f'checkpoints/best.weights.h5', save_weights_only=True, save_best_only=True, monitor='loss', mode='min', verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "learning_rate_start = 1e-1\n",
    "learning_rate_end = 1e-4\n",
    "\n",
    "lr_schedule_poly = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=learning_rate_start,\n",
    "    decay_steps=500,\n",
    "    end_learning_rate=learning_rate_end,\n",
    "    power=1.0\n",
    ")\n",
    "\n",
    "lr_schedule_exp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate_start,\n",
    "    decay_steps=500,\n",
    "    decay_rate= learning_rate_end / learning_rate_start,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "steps_per_epoch = EPOCH_SIZE // BATCH_SIZE\n",
    "lr_schedule_cycle = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=learning_rate_start,\n",
    "    maximal_learning_rate=learning_rate_end,\n",
    "    step_size=2 * steps_per_epoch,\n",
    "    scale_fn=lambda x: 1 / (2.0 ** (x - 1)),\n",
    "    scale_mode='cycle'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231499,
     "status": "ok",
     "timestamp": 1750255508586,
     "user": {
      "displayName": "Nino Rottier",
      "userId": "12165733875658235380"
     },
     "user_tz": -120
    },
    "id": "dh3DIWR1N4CC",
    "outputId": "b633c00e-100b-43b8-d867-d23285efbe8a"
   },
   "outputs": [],
   "source": [
    "# adam = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# adamW = tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "# rmsprop = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "# sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule_cycle)\n",
    "\n",
    "adam2 = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.5, beta_2=0.5, epsilon=1e-07, amsgrad=False)\n",
    "\n",
    "np.seterr(all='raise') # to check where nan are created\n",
    "tf.debugging.enable_check_numerics() # but it slows the code\n",
    "# nan_callback = tf.keras.callbacks.TerminateOnNaN() # essayer ça sinon\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam2,\n",
    "    loss=focal_dice_loss,\n",
    "    metrics=[f1_score_metric, dice_loss]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    gen,\n",
    "    epochs=50,\n",
    "    #callbacks=[checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "model.save('unet_model_from_normals.h5')\n",
    "\n",
    "# import gc\n",
    "# globals().clear()\n",
    "\n",
    "# # kill the kernel to free up memory\n",
    "# import os\n",
    "# os._exit(0)  # This will terminate the kernel and free up memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Courbes & prédictions des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1750256464603,
     "user": {
      "displayName": "Nino Rottier",
      "userId": "12165733875658235380"
     },
     "user_tz": -120
    },
    "id": "hpJFGK_WQQtS",
    "outputId": "6d3d3777-f88d-4ca4-d07f-28da7617e9bc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Récupération des données\n",
    "acc = history.history['f1_score_metric']\n",
    "loss = history.history['loss']\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "# Création des graphiques\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Précision\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train f1-score')\n",
    "plt.title('F1-score over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1-score')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Perte\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9409,
     "status": "ok",
     "timestamp": 1750256477050,
     "user": {
      "displayName": "Nino Rottier",
      "userId": "12165733875658235380"
     },
     "user_tz": -120
    },
    "id": "CMhqd0pbN6yK",
    "outputId": "8462bdd8-f4e6-4f00-850e-753307f0c8c4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Charger le modèle depuis le fichier .h5\n",
    "\n",
    "model = load_model('unet_model_from_normals.h5', custom_objects={'focal_dice_loss': focal_dice_loss, 'f1_score_metric': f1_score_metric, 'dice_loss': dice_loss})\n",
    "\n",
    "# Prédiction sur quelques images du générateur\n",
    "X_batch, y_batch = gen[0]  # Prend un batch\n",
    "# X_batch, y_batch = train_gen[0]  # Prend un batch\n",
    "preds = model.predict(X_batch)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(min(5, X_batch.shape[0])):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow((X_batch[i] + 1 )/2)  # Inverse normalization to [0, 1]\n",
    "    plt.title(\"Normal Map\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(y_batch[i], cmap='gray')\n",
    "    plt.title(\"Ground Truth\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(preds[i, ..., 0], cmap='gray')\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation sur des données réels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'image est redimensionnée à Height x Width\n",
    "IMG_PATH = './test_imgs/omoplate.png'\n",
    "MODEL_NAME = 'unet_model_from_normals.h5'\n",
    "MODEL_WEIGHTS = 'checkpoints/best.weights.h5'\n",
    "\n",
    "# charger le modèle avec les poids\n",
    "model = load_model(MODEL_NAME, custom_objects={'focal_dice_loss': focal_dice_loss, 'f1_score_metric': f1_score_metric, 'dice_loss': dice_loss})\n",
    "#model.load_weights(MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Charger et prétraiter la carte normale\n",
    "carte_normale = load_img(IMG_PATH, target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode=\"rgb\")\n",
    "carte_normale_arr = img_to_array(carte_normale) / 127.5 - 1\n",
    "carte_normale_arr = np.expand_dims(carte_normale_arr, axis=0)  # shape: (1, H, W, 3)\n",
    "\n",
    "# Prédiction du modèle\n",
    "pred = model.predict(carte_normale_arr)[0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(carte_normale)\n",
    "plt.title(\"Carte normale\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pred, cmap='gray')\n",
    "plt.title(\"Prediction\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
